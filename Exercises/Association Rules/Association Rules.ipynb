{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we will implement two techniques that are part of the so-called shopping basket analysis, which will help us to better understand how customers data are being processed to extract insights about their habits.\n",
    "\n",
    "\n",
    "#### Notes about external libraries\n",
    "You can check your implementation of the Apriori algorithm and the Association Rules using MLxtend, a data mining library. Unfortunately, the library is not directly shipped with Anaconda. To install MLxtend, just execute  \n",
    "\n",
    "```bash\n",
    "pip install mlxtend  \n",
    "```\n",
    "\n",
    "Or directly using Anaconda\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge mlxtend \n",
    "```\n",
    "\n",
    "Note that the installation of MLxtend is not mandatory, as we will provide the expected results in pre-rendered cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Exercise 1: Apriori algorithm\n",
    "In the first excercise, we will put into practice the Apriori algorithm. In particular, we will extract frequent itemsets from a list of transactions coming from a grocery store. You will have to complete the function `get_support(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Format the transaction dataset.\n",
    "Expect a list of transaction in the format:\n",
    "[[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], ...]\n",
    "\"\"\"\n",
    "def preprocess(dataset):\n",
    "    unique_items = set()\n",
    "    for transaction in dataset:\n",
    "        for item in transaction:\n",
    "            unique_items.add(item)\n",
    "       \n",
    "    # Converting to frozensets to use itemsets as dict key\n",
    "    unique_items = [frozenset([i]) for i in list(unique_items)]\n",
    "    \n",
    "    return unique_items,list(map(set,dataset))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Generate candidates of length n+1 from a list of items, each of length n.\n",
    "\n",
    "Example:\n",
    "[{1}, {2}, {5}]          -> [{1, 2}, {1, 5}, {2, 5}]\n",
    "[{2, 3}, {2, 5}, {3, 5}] -> [{2, 3, 5}]\n",
    "\"\"\"\n",
    "def generate_candidates(Lk):\n",
    "    output = []\n",
    "\n",
    "    # We generate rules of the target size k\n",
    "    k=len(Lk[0])+1\n",
    "    \n",
    "    for i in range(len(Lk)):\n",
    "        for j in range(i+1, len(Lk)): \n",
    "            L1 = list(Lk[i])[:k-2]; \n",
    "            L2 = list(Lk[j])[:k-2]\n",
    "            L1.sort(); \n",
    "            L2.sort()\n",
    "\n",
    "            # Merge sets if first k-2 elements are equal\n",
    "            # For the case of k<2, generate all possible combinations\n",
    "            if L1==L2: \n",
    "                output.append(Lk[i] | Lk[j])\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Print the results of the apriori algorithm\n",
    "\"\"\"\n",
    "def print_support(support,max_display=10,min_items=1):\n",
    "    print('support\\t itemset')\n",
    "    print('-'*30)\n",
    "    filt_support = {k:v for k,v in support.items() if len(k)>=min_items}\n",
    "    for s,sup in sorted(filt_support.items(), key=operator.itemgetter(1),reverse=True)[:max_display]:\n",
    "        print(\"%.2f\" % sup,'\\t',set(s))\n",
    "        \n",
    "def print_support_mx(df,max_display=10,min_items=1):\n",
    "    print('support\\t itemset')\n",
    "    print('-'*30)\n",
    "    lenrow = df['itemsets'].apply(lambda x: len(x))\n",
    "    df  = df[lenrow>=min_items]\n",
    "    df  = df.sort_values('support',ascending=False).iloc[:max_display]\n",
    "    for i,row in df.iterrows():\n",
    "        print(\"%.2f\" % float(row['support']),'\\t',set(row['itemsets']))\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "Run the apriori algorithm\n",
    "\n",
    "dataset     : list of transactions\n",
    "min_support : minimum support. Itemsets with support below this threshold\n",
    "              will be pruned.\n",
    "\"\"\"\n",
    "def apriori(dataset, min_support = 0.5):\n",
    "    unique_items,dataset = preprocess(dataset)\n",
    "    L1, supportData      = get_support(dataset, unique_items, min_support)\n",
    "    \n",
    "    L = [L1]\n",
    "    k = 0\n",
    "    while True:\n",
    "        Ck       = generate_candidates(L[k])\n",
    "        Lk, supK = get_support(dataset, Ck, min_support)\n",
    "        \n",
    "        # Is there itemsets of length k that have the minimum support ?\n",
    "        if len(Lk)>0:\n",
    "            supportData.update(supK)\n",
    "            L.append(Lk) \n",
    "            k += 1\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return L, supportData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "The [Apriori Algorithm](https://en.wikipedia.org/wiki/Apriori_algorithm) identifies frequent combinations of items by extending them to larger and larger itemsets (see the generate_candidates function) as long as they appear sufficiently often in a list of transactions.\n",
    "\n",
    "Compute support for all the candidate itemsets contained in Ck, given the total list of transactions. We already provide the functions to compute candidate itemsets. The support of the itemset $X$ with respect to the list of transactions $T$ is defined as the proportion of transactions $t$ in the dataset which contains the itemset $X$. Support can be computed using the following formula\n",
    "\n",
    "$$\\mathrm{supp}(X) = \\frac{|\\{t \\in T; X \\subseteq t\\}|}{|T|}$$  \n",
    "\n",
    "After computing the support for each itemset, prune the ones that do not match the minimal specificied support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute support for each provided itemset by counting the number of\n",
    "its occurences in the original dataset of transactions.\n",
    "\n",
    "dataset      : list of transactions, preprocessed using 'preprocess()'\n",
    "Ck           : list of itemsets to compute support for. \n",
    "min_support  : minimum support. Itemsets with support below this threshold\n",
    "               will be pruned.\n",
    "              \n",
    "output       : list of remaining itemsets, after the pruning step.\n",
    "support_dict : dictionary containing the support value for each itemset.\n",
    "\"\"\"\n",
    "def get_support(dataset, Ck, min_support):\n",
    "    \n",
    "    # This dictionary should contain the number of appearance of each itemset in the dataset.\n",
    "    # Itemset in Ck are represented as frozensets and can directly be uses as dictionary keys.\n",
    "    support_count = {}\n",
    "    \n",
    "    for transaction in dataset:\n",
    "        for candidate in Ck:\n",
    "            if candidate.issubset(transaction):\n",
    "                if not candidate in support_count: support_count[candidate]=1\n",
    "                else: support_count[candidate] += 1\n",
    "    \n",
    "    output = []\n",
    "    num_transactions = float(len(dataset))\n",
    "    support_dict = {}\n",
    "    for key in support_count:\n",
    "        \n",
    "        support = support_count[key]/num_transactions\n",
    "        \n",
    "        if support >= min_support:\n",
    "            output.insert(0,key)\n",
    "            support_dict[key] = support\n",
    "    return output, support_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support\t itemset\n",
      "------------------------------\n",
      "0.07 \t {'whole milk', 'other vegetables'}\n",
      "0.06 \t {'whole milk', 'rolls/buns'}\n",
      "0.06 \t {'whole milk', 'yogurt'}\n",
      "0.05 \t {'whole milk', 'root vegetables'}\n",
      "0.05 \t {'other vegetables', 'root vegetables'}\n",
      "0.04 \t {'other vegetables', 'yogurt'}\n",
      "0.04 \t {'other vegetables', 'rolls/buns'}\n",
      "0.04 \t {'whole milk', 'tropical fruit'}\n",
      "0.04 \t {'whole milk', 'soda'}\n",
      "0.04 \t {'rolls/buns', 'soda'}\n"
     ]
    }
   ],
   "source": [
    "dataset = [ l.strip().split(',') for i,l in enumerate(open('groceries.csv').readlines())]\n",
    "\n",
    "L,support = apriori(dataset,min_support=0.01)\n",
    "print_support(support,10,min_items=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check\n",
    "\n",
    "You can check the results of your implementation using MLXtend. Just run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlxtend\n",
      "  Obtaining dependency information for mlxtend from https://files.pythonhosted.org/packages/1c/07/512f6a780239ad6ce06ce2aa7b4067583f5ddcfc7703a964a082c706a070/mlxtend-0.23.1-py3-none-any.whl.metadata\n",
      "  Downloading mlxtend-0.23.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from mlxtend) (1.11.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from mlxtend) (1.26.0)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from mlxtend) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from mlxtend) (1.2.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from mlxtend) (3.7.2)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from mlxtend) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (9.4.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from pandas>=0.24.2->mlxtend) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from scikit-learn>=1.0.2->mlxtend) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\miniconda3\\envs\\dis\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
      "Downloading mlxtend-0.23.1-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.4 MB 1.5 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.2/1.4 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.4/1.4 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.6/1.4 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 0.8/1.4 MB 3.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.4 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.1/1.4 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.4/1.4 MB 3.8 MB/s eta 0:00:00\n",
      "Installing collected packages: mlxtend\n",
      "Successfully installed mlxtend-0.23.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sum() got an unexpected keyword argument 'level'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlxtend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrequent_patterns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apriori \u001b[38;5;28;01mas\u001b[39;00m mx_apriori\n\u001b[1;32m----> 4\u001b[0m df_dummy \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(pd\u001b[38;5;241m.\u001b[39mSeries(dataset)\u001b[38;5;241m.\u001b[39mapply(pd\u001b[38;5;241m.\u001b[39mSeries)\u001b[38;5;241m.\u001b[39mstack())\u001b[38;5;241m.\u001b[39msum(level \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m frequent_itemsets \u001b[38;5;241m=\u001b[39m mx_apriori(df_dummy, min_support\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, use_colnames\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m print_support_mx(frequent_itemsets,\u001b[38;5;241m10\u001b[39m,min_items\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\dis\\Lib\\site-packages\\pandas\\core\\generic.py:11512\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.sum\u001b[1;34m(self, axis, skipna, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m  11493\u001b[0m \u001b[38;5;129m@doc\u001b[39m(  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m  11494\u001b[0m     _num_doc,\n\u001b[0;32m  11495\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn the sum of the values over the requested axis.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11510\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11511\u001b[0m ):\n\u001b[1;32m> 11512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDFrame\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m, axis, skipna, numeric_only, min_count, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\dis\\Lib\\site-packages\\pandas\\core\\generic.py:11280\u001b[0m, in \u001b[0;36mNDFrame.sum\u001b[1;34m(self, axis, skipna, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m  11272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\n\u001b[0;32m  11273\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  11274\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11278\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11279\u001b[0m ):\n\u001b[1;32m> 11280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_min_count_stat_function(\n\u001b[0;32m  11281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnansum, axis, skipna, numeric_only, min_count, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m  11282\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\dis\\Lib\\site-packages\\pandas\\core\\generic.py:11252\u001b[0m, in \u001b[0;36mNDFrame._min_count_stat_function\u001b[1;34m(self, name, func, axis, skipna, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[0;32m  11240\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m  11241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_min_count_stat_function\u001b[39m(\n\u001b[0;32m  11242\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11249\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11250\u001b[0m ):\n\u001b[0;32m  11251\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m> 11252\u001b[0m         nv\u001b[38;5;241m.\u001b[39mvalidate_sum((), kwargs)\n\u001b[0;32m  11253\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprod\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m  11254\u001b[0m         nv\u001b[38;5;241m.\u001b[39mvalidate_prod((), kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\dis\\Lib\\site-packages\\pandas\\compat\\numpy\\function.py:82\u001b[0m, in \u001b[0;36mCompatValidator.__call__\u001b[1;34m(self, args, kwargs, fname, max_fname_arg_count, method)\u001b[0m\n\u001b[0;32m     80\u001b[0m     validate_kwargs(fname, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 82\u001b[0m     validate_args_and_kwargs(\n\u001b[0;32m     83\u001b[0m         fname, args, kwargs, max_fname_arg_count, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults\n\u001b[0;32m     84\u001b[0m     )\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid validation method \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\dis\\Lib\\site-packages\\pandas\\util\\_validators.py:221\u001b[0m, in \u001b[0;36mvalidate_args_and_kwargs\u001b[1;34m(fname, args, kwargs, max_fname_arg_count, compat_args)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    217\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() got multiple values for keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m         )\n\u001b[0;32m    220\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(args_dict)\n\u001b[1;32m--> 221\u001b[0m validate_kwargs(fname, kwargs, compat_args)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\dis\\Lib\\site-packages\\pandas\\util\\_validators.py:162\u001b[0m, in \u001b[0;36mvalidate_kwargs\u001b[1;34m(fname, kwargs, compat_args)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03mChecks whether parameters passed to the **kwargs argument in a\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03mfunction `fname` are valid parameters as specified in `*compat_args`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03mmap to the default values specified in `compat_args`\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m kwds \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 162\u001b[0m _check_for_invalid_keys(fname, kwargs, compat_args)\n\u001b[0;32m    163\u001b[0m _check_for_default_values(fname, kwds, compat_args)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\miniconda3\\envs\\dis\\Lib\\site-packages\\pandas\\util\\_validators.py:136\u001b[0m, in \u001b[0;36m_check_for_invalid_keys\u001b[1;34m(fname, kwargs, compat_args)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m diff:\n\u001b[0;32m    135\u001b[0m     bad_arg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(diff)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() got an unexpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbad_arg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: sum() got an unexpected keyword argument 'level'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori as mx_apriori\n",
    "\n",
    "df_dummy = pd.get_dummies(pd.Series(dataset).apply(pd.Series).stack()).sum(level = 0)\n",
    "frequent_itemsets = mx_apriori(df_dummy, min_support=0.01, use_colnames=True)\n",
    "print_support_mx(frequent_itemsets,10,min_items=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Exercise 2: Association Rule Learning\n",
    "Such associations are not necessarily symmetric. Therefore, in the second part, we will use [association rule learning](https://en.wikipedia.org/wiki/Association_rule_learning) to better understand the directionality of our computed frequent itemsets. In other terms, we will have to infer if the purchase of one item generally implies the the purchase of another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "L              : itemsets\n",
    "supportData    : dictionary storing itemsets support\n",
    "min_confidence : rules with a confidence under this threshold should be pruned\n",
    "\"\"\"\n",
    "def generate_rules(L, supportData, min_confidence=0.7):  \n",
    "    # Rules to be computed\n",
    "    rules = []\n",
    "    \n",
    "    # Iterate over itemsets of length 2..N\n",
    "    for i in range(1, len(L)):\n",
    "        \n",
    "        # Iterate over each frequent itemset\n",
    "        for freqSet in L[i]:\n",
    "            H1 = [frozenset([item]) for item in freqSet]\n",
    "            \n",
    "            # If the itemset contains more than 2 elements\n",
    "            # recursively generate candidates \n",
    "            if (i+1 > 2):\n",
    "                rules_from_consequent(freqSet, H1, supportData, rules, min_confidence)\n",
    "                compute_confidence(freqSet, H1, supportData, rules, min_confidence)\n",
    "            # If the itemsset contains 2 or less elements\n",
    "            # conpute rule confidence\n",
    "            else:\n",
    "                compute_confidence(freqSet, H1, supportData, rules, min_confidence)\n",
    "\n",
    "    return rules   \n",
    "\n",
    "\"\"\"\n",
    "freqSet        : frequent itemset\n",
    "H              : candidate elements to create a rule\n",
    "supportData    : dictionary storing itemsets support\n",
    "rules          : array to store rules\n",
    "min_confidence : rules with a confidence under this threshold should be pruned\n",
    "\"\"\"\n",
    "def rules_from_consequent(freqSet, H, supportData, rules, min_confidence=0.7):\n",
    "    m = len(H[0])\n",
    "    if (len(freqSet) > (m + 1)): \n",
    "\n",
    "        # create new candidates of size n+1\n",
    "        Hmp1 = generate_candidates(H)\n",
    "        Hmp1 = compute_confidence(freqSet, Hmp1, supportData, rules, min_confidence)\n",
    "        \n",
    "        if (len(Hmp1) > 1):    #need at least two sets to merge\n",
    "            rules_from_consequent(freqSet, Hmp1, supportData, rules, min_confidence)\n",
    "            \n",
    "\"\"\"\n",
    "Print the resulting rules\n",
    "\"\"\"\n",
    "def print_rules(rules,max_display=10):\n",
    "    print('confidence\\t rule')\n",
    "    print('-'*30)\n",
    "    for a,b,sup in sorted(rules, key=lambda x: x[2],reverse=True)[:max_display]:\n",
    "        print(\"%.2f\" % sup,'\\t',set(a),'->',set(b))\n",
    "def print_rules_mx(df,max_display=10):\n",
    "    print('confidence\\t rule')\n",
    "    print('-'*30)\n",
    "    df  = df.sort_values('confidence',ascending=False).iloc[:max_display]\n",
    "    for i,row in df.iterrows():\n",
    "        print(\"%.2f\" % float(row['confidence']),'\\t',set(row['antecedents']),'->',set(row['consequents']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "You will have to complete the method `compute_confidence(...)`, that computes confidence for a set of candidate rules H and prunes the rules that have a confidence below the specified threshold. Please complete it by computing rules confidence using the following formula:\n",
    "\n",
    "$$\\mathrm{conf}(X \\Rightarrow Y) = \\mathrm{supp}(X \\cup Y) / \\mathrm{supp}(X)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compute confidence for a given set of rules and their respective support\n",
    "\n",
    "freqSet        : frequent itemset of N-element\n",
    "H              : list of candidate elements Y1, Y2... that are part of the frequent itemset\n",
    "supportData    : dictionary storing itemsets support\n",
    "rules          : array to store rules\n",
    "min_confidence : rules with a confidence under this threshold should be pruned\n",
    "\"\"\"\n",
    "def compute_confidence(freqSet, H, supportData, rules, min_confidence=0.7):\n",
    "    prunedH = [] \n",
    "    \n",
    "    for Y in H:\n",
    "        # Compute X which is the frequent itemset minus the considered Y\n",
    "        X           = freqSet -  Y\n",
    "        \n",
    "        # Compute support for both terms\n",
    "        support_XuY = supportData[freqSet]\n",
    "        support_X   = supportData[X]\n",
    "        \n",
    "        # Compute confidence\n",
    "        conf        = support_XuY / support_X\n",
    "        \n",
    "        if conf >= min_confidence: \n",
    "            rules.append((X, Y, conf))\n",
    "            prunedH.append(Y)\n",
    "    return prunedH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence\t rule\n",
      "------------------------------\n",
      "0.59 \t {'citrus fruit', 'root vegetables'} -> {'other vegetables'}\n",
      "0.58 \t {'root vegetables', 'tropical fruit'} -> {'other vegetables'}\n",
      "0.58 \t {'curd', 'yogurt'} -> {'whole milk'}\n",
      "0.57 \t {'other vegetables', 'butter'} -> {'whole milk'}\n",
      "0.57 \t {'root vegetables', 'tropical fruit'} -> {'whole milk'}\n",
      "0.56 \t {'root vegetables', 'yogurt'} -> {'whole milk'}\n",
      "0.55 \t {'domestic eggs', 'other vegetables'} -> {'whole milk'}\n",
      "0.52 \t {'whipped/sour cream', 'yogurt'} -> {'whole milk'}\n",
      "0.52 \t {'rolls/buns', 'root vegetables'} -> {'whole milk'}\n",
      "0.52 \t {'other vegetables', 'pip fruit'} -> {'whole milk'}\n"
     ]
    }
   ],
   "source": [
    "rules=generate_rules(L,support, min_confidence=0.1)\n",
    "print_rules(rules,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check\n",
    "\n",
    "You can check the results of your implementation using MLXtend. Just run the cell below (you will have to run the checking code of question 1 first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frequent_itemsets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmlxtend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfrequent_patterns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m association_rules \u001b[38;5;28;01mas\u001b[39;00m mx_association_rules\n\u001b[1;32m----> 3\u001b[0m rules_mx \u001b[38;5;241m=\u001b[39m mx_association_rules(frequent_itemsets, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m\"\u001b[39m, min_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m      4\u001b[0m print_rules_mx(rules_mx,max_display\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'frequent_itemsets' is not defined"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import association_rules as mx_association_rules\n",
    "\n",
    "rules_mx = mx_association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.1)\n",
    "print_rules_mx(rules_mx,max_display=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPFL Twitter Data\n",
    "\n",
    "Now that we have a working implementation, we will apply the Apriori algorithm on a dataset that you should know pretty well by now: EPFL Twitter data. In that scenario, tweets will be considered as transactions and words will be items. Let's see what kind of frequent associations we can discover.\n",
    "\n",
    "The method below cleans the tweets and formats them in the same format as the transactions of the previous exercise. Run the cells and generate the results for both algorithms. What can you observe from the association rules results? Briefly explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Loading of libraries and documents\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Tokenize, stem a document\n",
    "stemmer = PorterStemmer()\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
    "\n",
    "# Remove stop words\n",
    "def clean_voc(documents):\n",
    "    cleaned = []\n",
    "    for tweet in documents:\n",
    "        new_tweet = []\n",
    "        tweet = tokenize(tweet).split()\n",
    "        for word in tweet:\n",
    "            if (word not in stopwords.words('english') and \n",
    "                word not in stopwords.words('german') and\n",
    "                word not in stopwords.words('french')):\n",
    "                if word==\"epflen\":\n",
    "                    word = \"epfl\"\n",
    "                new_tweet.append(word)\n",
    "        if len(new_tweet)>0:\n",
    "            cleaned.append(new_tweet)\n",
    "    return cleaned\n",
    "\n",
    "# Read a list of documents from a file. Each line in a file is a document\n",
    "with open(\"epfldocs.txt\") as f:\n",
    "    content = f.readlines()\n",
    "original_documents = [x.strip() for x in content] \n",
    "documents = clean_voc(original_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support\t itemset\n",
      "------------------------------\n",
      "0.08 \t {'via', 'epfl'}\n",
      "0.06 \t {'epfl', 'â€™'}\n",
      "0.05 \t {'new', 'epfl'}\n",
      "0.05 \t {'epfl', 'amp'}\n",
      "0.05 \t {'research', 'epfl'}\n",
      "0.04 \t {'lausann', 'epfl'}\n",
      "0.04 \t {'vdtech', 'epfl'}\n",
      "0.04 \t {'epfl', 'switzerland'}\n",
      "0.04 \t {'epfl', 'robot'}\n",
      "0.03 \t {'day', 'epfl'}\n",
      "0.03 \t {'epfl', 'swiss'}\n",
      "0.03 \t {'vdtech', 'via'}\n",
      "0.03 \t {'vdtech', 'via', 'epfl'}\n",
      "0.03 \t {'scienc', 'epfl'}\n",
      "0.03 \t {'epfl', 'innov'}\n",
      "0.03 \t {'epfl', 'student'}\n",
      "0.03 \t {'epfl', 'first'}\n",
      "0.03 \t {'work', 'epfl'}\n",
      "0.02 \t {'technolog', 'epfl'}\n",
      "0.02 \t {'epfl', '2018'}\n"
     ]
    }
   ],
   "source": [
    "L,support = apriori(documents,min_support = 0.01)\n",
    "print_support(support,20,min_items=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence\t rule\n",
      "------------------------------\n",
      "1.00 \t {'Â»'} -> {'epfl'}\n",
      "1.00 \t {'Â«'} -> {'epfl'}\n",
      "1.00 \t {'Â«'} -> {'Â»'}\n",
      "1.00 \t {'Â»'} -> {'Â«'}\n",
      "1.00 \t {'model'} -> {'epfl'}\n",
      "1.00 \t {'perovskit'} -> {'epfl'}\n",
      "1.00 \t {'next'} -> {'epfl'}\n",
      "1.00 \t {'improv'} -> {'epfl'}\n",
      "1.00 \t {'particip'} -> {'epfl'}\n",
      "1.00 \t {'technolog'} -> {'epfl'}\n",
      "1.00 \t {'drone'} -> {'epfl'}\n",
      "1.00 \t {'epflcampu'} -> {'epfl'}\n",
      "1.00 \t {'learn'} -> {'epfl'}\n",
      "1.00 \t {'present'} -> {'epfl'}\n",
      "1.00 \t {'mooc'} -> {'epfl'}\n",
      "1.00 \t {'show'} -> {'epfl'}\n",
      "1.00 \t {'scientist'} -> {'epfl'}\n",
      "1.00 \t {'brain'} -> {'epfl'}\n",
      "1.00 \t {'eth'} -> {'epfl'}\n",
      "1.00 \t {'Â«'} -> {'Â»', 'epfl'}\n"
     ]
    }
   ],
   "source": [
    "rules=generate_rules(L,support, min_confidence=0.1)\n",
    "print_rules(rules,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Exercise 3: Pen and Paper!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are given the following accident and weather data. Each line corresponds to one event:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. car_accident rain lightning wind clouds fire\n",
    "2. fire clouds rain lightning wind\n",
    "3. car_accident fire wind\n",
    "4. clouds rain wind\n",
    "5. lightning fire rain clouds  \n",
    "6. clouds wind car_accident  \n",
    "7. rain lightning clouds fire  \n",
    "8. lightning fire car_accident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) You would like to know what is the likely cause of all the car accidents. What association rules do you need to look for? Compute the confidence and support values for these rules. Looking at these values, which is the most likely cause of the car accidents?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to look for the association rules of the form: {cause} â†’ {car accident}\n",
    "i.e. in which the left-hand side represents the cause of the accident. \n",
    "\n",
    "The possible association rules are: \n",
    "\n",
    "{lightning} â†’ {car accident} support: 0.25 confidence: 0.4\n",
    "    Support: 0.25 means that lightning and car accident is present in 25% = 2 of all transactions.\n",
    "    Confidence: 0.4 means that among the transactions involving lightning, 40% of them also involve a car accident. = number total of transactions with lightning + car accident / number total of transactions with lightning = 2/5 = 0.4\n",
    "\n",
    "{wind} â†’ {car accident} support: 0.375 confidence: 0.6\n",
    "    Support: 0.375 indicates that wind + car accident is present in 37.5% = 3 of all transactions.\n",
    "    Confidence: 0.6 = number total of transactions with wind + car accident / number total of transactions with wind = 3/5 = 0.6\n",
    "\n",
    "{fire} â†’ {car accident} support: 0.375 confidence: 0.5\n",
    "    support: there is 3 transactions with fire + car accident; 3/8 = 0.375\n",
    "    confidence: number total of transactions with fire + car accident / number total of transactions with fire = 3/6 = 0.5\n",
    "\n",
    "{clouds} â†’ {car accident} support: 0.25 confidence: 0.33\n",
    "    support: there is 2 transactions with clouds + car accident; 2/8 = 0.25\n",
    "    confidence: number total of transactions with clouds + car accident / number total of transactions with clouds = 2/6 = 0.33\n",
    "    \n",
    "{rain} â†’ {car accident} support: 0.125 confidence: 0.2\n",
    "    support: there is 1 transactions with rain + car accident; 1/8 = 0.125\n",
    "    confidence: number total of transactions with rain + car accident / number total of transactions with clouds = 1/5 = 0.2\n",
    "\n",
    "\n",
    "{wind} has both the highest confidence and the highest support and is the most likely cause of the car accidents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Find all the association rules for minimal support 0.6 and minimal confidence of 1.0 (certainty). Follow the apriori algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first find all the frequent itemsets of size one. The minimal support requirement is 0.6,\n",
    "which means that to be frequent an itemset must occur in at least 5 out of the 8 transactions, 5/8 = 0.625 â‰¥ 0.6 There are five frequent itemsets:\n",
    "\n",
    "{clouds} support: 0.75\n",
    "{wind} support: 0.625\n",
    "{lightning} support: 0.625\n",
    "{rain} support: 0.625\n",
    "{fire} support: 0.75\n",
    "\n",
    "From the above itemsets we next generate all possible itemsets of size 2 and prune the itemsets with support below 0.6. Only two itemsets remain:\n",
    "\n",
    "{lightning, fire} support: 0.625\n",
    "{clouds, rain} support: 0.625\n",
    "\n",
    "\n",
    "It is not possible to generate the itemsets of size 3 out of the above 2 itemsets, the intersection is empty. \n",
    "\n",
    "Based on the itemsets of size 2 we generate all possible association rules and compute their confidence:\n",
    "\n",
    "{lightning} â†’{fire} support: 0.625 confidence: 1.0\n",
    "{fire} â†’ {lightning} support: 0.625 confidence: 0.833\n",
    "{clouds} â†’ {rain} support: 0.625 confidence: 0.833\n",
    "{rain} â†’ {clouds} support: 0.625 confidence: 1.0\n",
    "\n",
    "There are only two association rules with confidence equal to 1 and that is the final solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# l'ordre d'affichage dans la liste est important : rain --> cloud c'est pas pareil que cloud --> rain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
